{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Based Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Chaining\n",
    "\n",
    "# Prompt Chaining is a technique where you chain multiple prompts together to create a more complex and detailed response.\n",
    "\n",
    "# To improve the reliability and performance of LLMs, one of the important prompt engineering techniques is to break tasks into its subtasks. Once those subtasks have been identified, the LLM is prompted with a subtask and then its response is used as input to another prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not have access to real-time information, including current weather conditions.  To get the current weather in Tokyo, please check a reliable weather website or app such as Google Weather, AccuWeather, or your local news.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"What is the weather in Tokyo?\")\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Sanitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Prompt 0: alcohol for teens\n",
      "The legal drinking age in most countries is 18 or 21.  Providing alcohol to minors is illegal and can have serious consequences.  There are many significant health risks associated with underage drinking, including:\n",
      "\n",
      "* **Brain damage:**  The brain is still developing during adolescence, and alcohol can disrupt this process, potentially leading to long-term cognitive impairments.\n",
      "* **Increased risk of alcohol dependence:**  Starting to drink at a young age significantly increases the likelihood of developing alcohol dependence or alcohol use disorder later in life.\n",
      "* **Physical health problems:**  Underage drinking can lead to liver damage, heart problems, and other health issues.\n",
      "* **Mental health problems:**  Alcohol can worsen existing mental health conditions and increase the risk of developing new ones, such as depression and anxiety.\n",
      "* **Increased risk of accidents and injuries:**  Alcohol impairs judgment and coordination, leading to a higher risk of accidents, injuries, and even death.\n",
      "* **Risky sexual behavior:**  Alcohol consumption can lead to risky sexual behavior and increase the risk of sexually transmitted infections (STIs).\n",
      "* **Academic problems:**  Underage drinking can negatively impact academic performance and school attendance.\n",
      "\n",
      "\n",
      "If you or someone you know is struggling with underage drinking, it's important to seek help.  Resources are available to provide support and guidance.  You can contact a trusted adult, such as a parent, teacher, or counselor, or search online for resources in your area.  Many organizations offer confidential help and support for young people and their families dealing with alcohol issues.\n",
      "Prompt 1: Output the response alcohol for teens in a way that is safe for kids learning any bad response should be avoided\n",
      "Alcohol is not safe for teens.  Their bodies and brains are still developing, and alcohol can seriously harm them.  It can affect their growth, learning, and even their ability to make good decisions.  Drinking alcohol at a young age increases the risk of problems later in life, including addiction, health issues, and accidents.  It's important to wait until you are a legal adult to drink alcohol, if you choose to drink at all.  If you or someone you know is struggling with alcohol, it's important to talk to a trusted adult, like a parent, teacher, or counselor.\n"
     ]
    }
   ],
   "source": [
    "n=input(\"Enter the question: \")\n",
    "\n",
    "list_of_prompts = [\n",
    "    n,\n",
    "    f\"Output the response {n} in a way that is safe for kids learning any bad response should be avoided\"\n",
    "]\n",
    "n=len(list_of_prompts)\n",
    "print(n)\n",
    "for i,j in enumerate(list_of_prompts):\n",
    "    print(f\"Prompt {i}: {j}\")\n",
    "    llm_response = llm.invoke(j)\n",
    "    print(llm_response.content)\n",
    "    if i==n:\n",
    "        print(\"Final response: \", llm_response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import Annotated, TypedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response(BaseModel):\n",
    "    \"Answer to the question\"\n",
    "    answer:Annotated[str, Field(description=\"The answer to the question\")]\n",
    "    rating:Annotated[int, Field(description=\"The rating of the answer from 1 to 10\")]\n",
    "    safety_rating:Annotated[int, Field(description=\"The safety rating of the answer from 1 to 10 for kids, 10 being the safest\")]\n",
    "    \n",
    "\n",
    "structured_llm=llm.with_structured_output(Response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A land of ancient wisdom, old and grand,\n",
      "Where vibrant colors paint the sacred land.\n",
      "From soaring Himalayas to the ocean's shore,\n",
      "A tapestry of cultures, evermore.\n",
      "\n",
      "The Ganges flows, a lifeline, pure and deep,\n",
      "While spices scent the air, secrets to keep.\n",
      "The Taj Mahal, a teardrop, white and bright,\n",
      "A monument to love, a stunning sight.\n",
      "\n",
      "From bustling markets to serene retreats,\n",
      "India's spirit in the soul it meets.\n",
      "A symphony of sounds, a vibrant hue,\n",
      "A land of magic, ever strong and true.\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "response=structured_llm.invoke(\"poem on India\")\n",
    "\n",
    "print(response.answer)\n",
    "print(response.rating)\n",
    "print(response.safety_rating)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=input(\"Enter the question: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Poem on Wine\n",
      "1 Output the response Poem on Wine in a way that is safe for kids learning any bad response should be avoided\n"
     ]
    }
   ],
   "source": [
    "# Response sanitization \n",
    "\n",
    "list_of_prompts = [\n",
    "    n,\n",
    "    f\"Output the response {n} in a way that is safe for kids learning any bad response should be avoided\"\n",
    "]\n",
    "\n",
    "for i,j in enumerate(list_of_prompts):\n",
    "    print(i,j)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
